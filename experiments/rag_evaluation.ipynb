{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Evaluation with DeepEval Framework\n",
        "\n",
        "This notebook evaluates the Medical RAG chatbot using DeepEval framework.\n",
        "\n",
        "## Evaluation Metrics:\n",
        "1. **FaithfulnessMetric**: Checks if the answer is grounded in the retrieved context\n",
        "2. **AnswerRelevancyMetric**: Checks if the answer is relevant to the question\n",
        "3. **ContextualPrecisionMetric**: Measures precision of retrieved context\n",
        "4. **ContextualRecallMetric**: Measures recall of retrieved context\n",
        "5. **ContextualRelevancyMetric**: Checks if retrieved context is relevant to the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('../')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepeval in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (3.13.3)\n",
            "Requirement already satisfied: click<8.3.0,>=8.0.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (8.2.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.76.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (3.1.4)\n",
            "Requirement already satisfied: nest_asyncio in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.6.0)\n",
            "Requirement already satisfied: openai in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.109.1)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.39.1)\n",
            "Requirement already satisfied: portalocker in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (3.2.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=5.4.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (5.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.11.7 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (2.12.5)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (2.12.0)\n",
            "Requirement already satisfied: pyfiglet in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.0.4)\n",
            "Requirement already satisfied: pytest in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (8.4.2)\n",
            "Requirement already satisfied: pytest-asyncio in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.3.0)\n",
            "Requirement already satisfied: pytest-repeat in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (0.9.4)\n",
            "Requirement already satisfied: pytest-rerunfailures in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (16.1)\n",
            "Requirement already satisfied: pytest-xdist in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (3.8.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.1.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (1.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (2.32.5)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.6.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (14.2.0)\n",
            "Requirement already satisfied: sentry-sdk in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (2.50.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (80.9.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: tenacity<=10.0.0,>=8.0.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (8.5.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (4.67.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (0.21.1)\n",
            "Requirement already satisfied: wheel in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from deepeval) (0.45.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from click<8.3.0,>=8.0.0->deepeval) (0.4.6)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from grpcio<2.0.0,>=1.67.1->deepeval) (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.1)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.39.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.39.1)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from opentelemetry-proto==1.39.1->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (6.33.2)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.60b1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from posthog<6.0.0,>=5.4.0->deepeval) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from posthog<6.0.0,>=5.4.0->deepeval) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from posthog<6.0.0,>=5.4.0->deepeval) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from posthog<6.0.0,>=5.4.0->deepeval) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from requests<3.0.0,>=2.31.0->deepeval) (2026.1.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from rich<15.0.0,>=13.6.0->deepeval) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from rich<15.0.0,>=13.6.0->deepeval) (2.19.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.6.0->deepeval) (0.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from aiohttp->deepeval) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from aiohttp->deepeval) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from aiohttp->deepeval) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from aiohttp->deepeval) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from aiohttp->deepeval) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from aiohttp->deepeval) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from aiohttp->deepeval) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from jinja2->deepeval) (3.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from openai->deepeval) (4.12.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from openai->deepeval) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from openai->deepeval) (0.12.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from openai->deepeval) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from httpx<1,>=0.23.0->openai->deepeval) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->deepeval) (0.16.0)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from portalocker->deepeval) (311)\n",
            "Requirement already satisfied: iniconfig>=1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from pytest->deepeval) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from pytest->deepeval) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from pytest->deepeval) (1.6.0)\n",
            "Requirement already satisfied: execnet>=2.1 in c:\\users\\suvan\\anaconda3\\envs\\hair_env\\lib\\site-packages (from pytest-xdist->deepeval) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install deepeval if not already installed\n",
        "!pip install deepeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# DeepEval imports\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import (\n",
        "    FaithfulnessMetric,\n",
        "    AnswerRelevancyMetric,\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    ContextualRelevancyMetric\n",
        ")\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up environment variables\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing embeddings...\n",
            "Connecting to Pinecone index...\n",
            "Creating retriever...\n",
            "Initializing chat model...\n",
            "Setting up prompt...\n",
            "Creating RAG chain...\n",
            "RAG system initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Download embeddings function (as in trials.ipynb)\n",
        "def download_embeddings():\n",
        "    \"\"\"\n",
        "    Download and return the HuggingFace embeddings model.\n",
        "    \"\"\"\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=model_name\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "# Initialize RAG components\n",
        "print(\"Initializing embeddings...\")\n",
        "embedding = download_embeddings()\n",
        "\n",
        "print(\"Connecting to Pinecone index...\")\n",
        "index_name = \"medical-chatbot\"\n",
        "docsearch = PineconeVectorStore.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embedding\n",
        ")\n",
        "\n",
        "print(\"Creating retriever...\")\n",
        "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "print(\"Initializing chat model...\")\n",
        "chatModel = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "print(\"Setting up prompt...\")\n",
        "system_prompt = (\n",
        "    \"You are an Medical assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Creating RAG chain...\")\n",
        "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "print(\"RAG system initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Test Cases\n",
        "\n",
        "Create test cases with medical questions that should be answerable from the medical book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined 8 test questions\n"
          ]
        }
      ],
      "source": [
        "# Define test questions\n",
        "test_questions = [\n",
        "    \"What is Acne?\",\n",
        "    \"What is the treatment of Acne?\",\n",
        "    \"What is Acromegaly and gigantism?\",\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"What is hypertension?\",\n",
        "    \"How is pneumonia treated?\",\n",
        "    \"What causes asthma?\",\n",
        "    \"What are the side effects of common medications?\"\n",
        "]\n",
        "\n",
        "answers = [\"Acne is a common skin disorder caused by blocked hair follicles due to excess oil, dead skin cells, and bacteria. It commonly appears as pimples, blackheads, or cysts, especially on the face, chest, and back.\",\n",
        "\"Treatment of acne includes topical agents like benzoyl peroxide or retinoids, oral antibiotics for infection, and hormonal therapy in severe cases. Proper skin hygiene also helps reduce outbreaks.\",\n",
        "\"Acromegaly and gigantism are hormonal disorders caused by excessive growth hormone secretion, usually from a pituitary tumor. Gigantism occurs in children before bone growth stops, while acromegaly affects adults.\",\n",
        "\"Common symptoms of diabetes include excessive thirst, frequent urination, unexplained weight loss, fatigue, and blurred vision. Poor wound healing and recurrent infections may also occur.\",\n",
        "\"Hypertension is a condition characterized by persistently high blood pressure in the arteries. It increases the risk of heart disease, stroke, and kidney failure if left untreated.\",\n",
        "\"Pneumonia is treated using antibiotics for bacterial infections, along with rest, fluids, and oxygen therapy if needed. Severe cases may require hospitalization and intravenous medication.\",\n",
        "\"Asthma is caused by airway inflammation and hyper-responsiveness triggered by allergens, infections, exercise, or environmental pollutants. Genetic and environmental factors both play a role. \",\n",
        "\"Common medication side effects include nausea, dizziness, headache, allergic reactions, and gastrointestinal discomfort. The severity depends on the drug type, dosage, and patient sensitivity.\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(test_questions)} test questions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get RAG response and retrieval context\n",
        "def get_rag_response_and_context(question):\n",
        "    \"\"\"Get answer and retrieval context from RAG system.\"\"\"\n",
        "    # Get retrieval context\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    retrieval_context = [doc.page_content for doc in retrieved_docs]\n",
        "    \n",
        "    # Get RAG response\n",
        "    response = rag_chain.invoke({\"input\": question})\n",
        "    answer = response.get(\"answer\", \"\")\n",
        "    \n",
        "    return answer, retrieval_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating test cases...\n",
            "Processing: What is Acne?\n",
            "✓ Generated test case for: What is Acne?...\n",
            "Processing: What is the treatment of Acne?\n",
            "✓ Generated test case for: What is the treatment of Acne?...\n",
            "Processing: What is Acromegaly and gigantism?\n",
            "✓ Generated test case for: What is Acromegaly and gigantism?...\n",
            "Processing: What are the symptoms of diabetes?\n",
            "✓ Generated test case for: What are the symptoms of diabetes?...\n",
            "Processing: What is hypertension?\n",
            "✓ Generated test case for: What is hypertension?...\n",
            "Processing: How is pneumonia treated?\n",
            "✓ Generated test case for: How is pneumonia treated?...\n",
            "Processing: What causes asthma?\n",
            "✓ Generated test case for: What causes asthma?...\n",
            "Processing: What are the side effects of common medications?\n",
            "✓ Generated test case for: What are the side effects of common medications?...\n",
            "\n",
            "Generated 8 test cases\n"
          ]
        }
      ],
      "source": [
        "# Generate test cases\n",
        "print(\"Generating test cases...\")\n",
        "test_cases = []\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"Processing: {question}\")\n",
        "    actual_output, retrieval_context = get_rag_response_and_context(question)\n",
        "    \n",
        "    test_case = LLMTestCase(\n",
        "        input=question,\n",
        "        actual_output=actual_output,\n",
        "        retrieval_context=retrieval_context\n",
        "    )\n",
        "    test_cases.append(test_case)\n",
        "    print(f\"✓ Generated test case for: {question[:50]}...\")\n",
        "\n",
        "print(f\"\\nGenerated {len(test_cases)} test cases\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating test cases with expected_output...\n",
            "✓ Added expected_output to test case 1: What is Acne?...\n",
            "✓ Added expected_output to test case 2: What is the treatment of Acne?...\n",
            "✓ Added expected_output to test case 3: What is Acromegaly and gigantism?...\n",
            "✓ Added expected_output to test case 4: What are the symptoms of diabetes?...\n",
            "✓ Added expected_output to test case 5: What is hypertension?...\n",
            "✓ Added expected_output to test case 6: How is pneumonia treated?...\n",
            "✓ Added expected_output to test case 7: What causes asthma?...\n",
            "✓ Added expected_output to test case 8: What are the side effects of common medications?...\n",
            "\n",
            "Updated 8 test cases with expected_output\n"
          ]
        }
      ],
      "source": [
        "# Update test cases with expected_output (ground truth answers)\n",
        "print(\"Updating test cases with expected_output...\")\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    if i < len(answers):\n",
        "        test_case.expected_output = answers[i]\n",
        "        print(f\"✓ Added expected_output to test case {i+1}: {test_questions[i][:50]}...\")\n",
        "    else:\n",
        "        print(f\"⚠ No expected_output available for test case {i+1}\")\n",
        "\n",
        "print(f\"\\nUpdated {len([tc for tc in test_cases if tc.expected_output])} test cases with expected_output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Test Case:\n",
            "Input: What is Acne?\n",
            "Output: Acne is a skin disorder where the sebaceous glands become inflamed, leading to the development of pimples and other lesions. It is commonly referred to as acne vulgaris....\n",
            "Retrieval Context (first chunk): GALE ENCYCLOPEDIA OF MEDICINE 226\n",
            "Acne\n",
            "GEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26...\n"
          ]
        }
      ],
      "source": [
        "# Display sample test case\n",
        "if test_cases:\n",
        "    print(\"Sample Test Case:\")\n",
        "    print(f\"Input: {test_cases[0].input}\")\n",
        "    print(f\"Output: {test_cases[0].actual_output[:200]}...\")\n",
        "    print(f\"Retrieval Context (first chunk): {test_cases[0].retrieval_context[0][:200] if test_cases[0].retrieval_context else 'None'}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Test Cases for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 8 test cases ready for evaluation\n"
          ]
        }
      ],
      "source": [
        "# Test cases are ready for evaluation\n",
        "print(f\"Created {len(test_cases)} test cases ready for evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized evaluation metrics:\n",
            "  - FaithfulnessMetric\n",
            "  - AnswerRelevancyMetric\n",
            "  - ContextualPrecisionMetric\n",
            "  - ContextualRecallMetric\n",
            "  - ContextualRelevancyMetric\n"
          ]
        }
      ],
      "source": [
        "# Initialize metrics\n",
        "faithfulness_metric = FaithfulnessMetric(threshold=0.7)\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
        "contextual_precision_metric = ContextualPrecisionMetric(threshold=0.7)\n",
        "contextual_recall_metric = ContextualRecallMetric(threshold=0.7)\n",
        "contextual_relevancy_metric = ContextualRelevancyMetric(threshold=0.7)\n",
        "\n",
        "metrics = [\n",
        "    faithfulness_metric,\n",
        "    answer_relevancy_metric,\n",
        "    contextual_precision_metric,\n",
        "    contextual_recall_metric,\n",
        "    contextual_relevancy_metric\n",
        "]\n",
        "\n",
        "print(\"Initialized evaluation metrics:\")\n",
        "for metric in metrics:\n",
        "    print(f\"  - {metric.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation...\n",
            "This may take a few minutes...\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "evaluate() got an unexpected keyword argument 'skip_on_missing_params'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take a few minutes...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Safety parameter\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Evaluation completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: evaluate() got an unexpected keyword argument 'skip_on_missing_params'"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "print(\"Starting evaluation...\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "evaluate(\n",
        "    test_cases=test_cases,\n",
        "    metrics=metrics,\n",
        "    skip_on_missing_params=True  # Safety parameter\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Evaluation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Individual Metric Evaluation\n",
        "\n",
        "Evaluate each metric individually to get detailed results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Faithfulness Metric\n",
        "print(\"Evaluating Faithfulness Metric...\")\n",
        "faithfulness_results = []\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    try:\n",
        "        faithfulness_metric.measure(test_case)\n",
        "        faithfulness_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': faithfulness_metric.score,\n",
        "            'reason': faithfulness_metric.reason,\n",
        "            'success': faithfulness_metric.success\n",
        "        })\n",
        "        print(f\"  Test {i+1}: {test_case.input[:50]}... - Score: {faithfulness_metric.score:.3f} - {'✓' if faithfulness_metric.success else '✗'}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Test {i+1}: Error - {str(e)}\")\n",
        "        faithfulness_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': None,\n",
        "            'reason': str(e),\n",
        "            'success': False\n",
        "        })\n",
        "\n",
        "print(f\"\\nFaithfulness Average Score: {sum([r['score'] for r in faithfulness_results if r['score'] is not None]) / len([r for r in faithfulness_results if r['score'] is not None]):.3f}\")\n",
        "print(f\"Success Rate: {sum([1 for r in faithfulness_results if r['success']]) / len(faithfulness_results) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Answer Relevancy Metric\n",
        "print(\"Evaluating Answer Relevancy Metric...\")\n",
        "answer_relevancy_results = []\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    try:\n",
        "        answer_relevancy_metric.measure(test_case)\n",
        "        answer_relevancy_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': answer_relevancy_metric.score,\n",
        "            'reason': answer_relevancy_metric.reason,\n",
        "            'success': answer_relevancy_metric.success\n",
        "        })\n",
        "        print(f\"  Test {i+1}: {test_case.input[:50]}... - Score: {answer_relevancy_metric.score:.3f} - {'✓' if answer_relevancy_metric.success else '✗'}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Test {i+1}: Error - {str(e)}\")\n",
        "        answer_relevancy_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': None,\n",
        "            'reason': str(e),\n",
        "            'success': False\n",
        "        })\n",
        "\n",
        "print(f\"\\nAnswer Relevancy Average Score: {sum([r['score'] for r in answer_relevancy_results if r['score'] is not None]) / len([r for r in answer_relevancy_results if r['score'] is not None]):.3f}\")\n",
        "print(f\"Success Rate: {sum([1 for r in answer_relevancy_results if r['success']]) / len(answer_relevancy_results) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Contextual Precision Metric\n",
        "print(\"Evaluating Contextual Precision Metric...\")\n",
        "contextual_precision_results = []\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    try:\n",
        "        contextual_precision_metric.measure(test_case)\n",
        "        contextual_precision_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': contextual_precision_metric.score,\n",
        "            'reason': contextual_precision_metric.reason,\n",
        "            'success': contextual_precision_metric.success\n",
        "        })\n",
        "        print(f\"  Test {i+1}: {test_case.input[:50]}... - Score: {contextual_precision_metric.score:.3f} - {'✓' if contextual_precision_metric.success else '✗'}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Test {i+1}: Error - {str(e)}\")\n",
        "        contextual_precision_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': None,\n",
        "            'reason': str(e),\n",
        "            'success': False\n",
        "        })\n",
        "\n",
        "print(f\"\\nContextual Precision Average Score: {sum([r['score'] for r in contextual_precision_results if r['score'] is not None]) / len([r for r in contextual_precision_results if r['score'] is not None]):.3f}\")\n",
        "print(f\"Success Rate: {sum([1 for r in contextual_precision_results if r['success']]) / len(contextual_precision_results) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Contextual Recall Metric\n",
        "print(\"Evaluating Contextual Recall Metric...\")\n",
        "contextual_recall_results = []\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    try:\n",
        "        contextual_recall_metric.measure(test_case)\n",
        "        contextual_recall_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': contextual_recall_metric.score,\n",
        "            'reason': contextual_recall_metric.reason,\n",
        "            'success': contextual_recall_metric.success\n",
        "        })\n",
        "        print(f\"  Test {i+1}: {test_case.input[:50]}... - Score: {contextual_recall_metric.score:.3f} - {'✓' if contextual_recall_metric.success else '✗'}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Test {i+1}: Error - {str(e)}\")\n",
        "        contextual_recall_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': None,\n",
        "            'reason': str(e),\n",
        "            'success': False\n",
        "        })\n",
        "\n",
        "print(f\"\\nContextual Recall Average Score: {sum([r['score'] for r in contextual_recall_results if r['score'] is not None]) / len([r for r in contextual_recall_results if r['score'] is not None]):.3f}\")\n",
        "print(f\"Success Rate: {sum([1 for r in contextual_recall_results if r['success']]) / len(contextual_recall_results) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Contextual Relevancy Metric\n",
        "print(\"Evaluating Contextual Relevancy Metric...\")\n",
        "contextual_relevancy_results = []\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    try:\n",
        "        contextual_relevancy_metric.measure(test_case)\n",
        "        contextual_relevancy_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': contextual_relevancy_metric.score,\n",
        "            'reason': contextual_relevancy_metric.reason,\n",
        "            'success': contextual_relevancy_metric.success\n",
        "        })\n",
        "        print(f\"  Test {i+1}: {test_case.input[:50]}... - Score: {contextual_relevancy_metric.score:.3f} - {'✓' if contextual_relevancy_metric.success else '✗'}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Test {i+1}: Error - {str(e)}\")\n",
        "        contextual_relevancy_results.append({\n",
        "            'question': test_case.input,\n",
        "            'score': None,\n",
        "            'reason': str(e),\n",
        "            'success': False\n",
        "        })\n",
        "\n",
        "print(f\"\\nContextual Relevancy Average Score: {sum([r['score'] for r in contextual_relevancy_results if r['score'] is not None]) / len([r for r in contextual_relevancy_results if r['score'] is not None]):.3f}\")\n",
        "print(f\"Success Rate: {sum([1 for r in contextual_relevancy_results if r['success']]) / len(contextual_relevancy_results) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary report\n",
        "import pandas as pd\n",
        "\n",
        "summary_data = {\n",
        "    'Metric': [\n",
        "        'Faithfulness',\n",
        "        'Answer Relevancy',\n",
        "        'Contextual Precision',\n",
        "        'Contextual Recall',\n",
        "        'Contextual Relevancy'\n",
        "    ],\n",
        "    'Average Score': [\n",
        "        sum([r['score'] for r in faithfulness_results if r['score'] is not None]) / len([r for r in faithfulness_results if r['score'] is not None]) if any(r['score'] is not None for r in faithfulness_results) else 0,\n",
        "        sum([r['score'] for r in answer_relevancy_results if r['score'] is not None]) / len([r for r in answer_relevancy_results if r['score'] is not None]) if any(r['score'] is not None for r in answer_relevancy_results) else 0,\n",
        "        sum([r['score'] for r in contextual_precision_results if r['score'] is not None]) / len([r for r in contextual_precision_results if r['score'] is not None]) if any(r['score'] is not None for r in contextual_precision_results) else 0,\n",
        "        sum([r['score'] for r in contextual_recall_results if r['score'] is not None]) / len([r for r in contextual_recall_results if r['score'] is not None]) if any(r['score'] is not None for r in contextual_recall_results) else 0,\n",
        "        sum([r['score'] for r in contextual_relevancy_results if r['score'] is not None]) / len([r for r in contextual_relevancy_results if r['score'] is not None]) if any(r['score'] is not None for r in contextual_relevancy_results) else 0\n",
        "    ],\n",
        "    'Success Rate (%)': [\n",
        "        sum([1 for r in faithfulness_results if r['success']]) / len(faithfulness_results) * 100,\n",
        "        sum([1 for r in answer_relevancy_results if r['success']]) / len(answer_relevancy_results) * 100,\n",
        "        sum([1 for r in contextual_precision_results if r['success']]) / len(contextual_precision_results) * 100,\n",
        "        sum([1 for r in contextual_recall_results if r['success']]) / len(contextual_recall_results) * 100,\n",
        "        sum([1 for r in contextual_relevancy_results if r['success']]) / len(contextual_relevancy_results) * 100\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION SUMMARY REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detailed results for each test case\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DETAILED RESULTS BY TEST CASE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    print(f\"\\nTest Case {i+1}: {test_case.input}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    if i < len(faithfulness_results):\n",
        "        print(f\"Faithfulness: {faithfulness_results[i]['score']:.3f if faithfulness_results[i]['score'] else 'N/A'} - {'✓' if faithfulness_results[i]['success'] else '✗'}\")\n",
        "    \n",
        "    if i < len(answer_relevancy_results):\n",
        "        print(f\"Answer Relevancy: {answer_relevancy_results[i]['score']:.3f if answer_relevancy_results[i]['score'] else 'N/A'} - {'✓' if answer_relevancy_results[i]['success'] else '✗'}\")\n",
        "    \n",
        "    if i < len(contextual_precision_results):\n",
        "        print(f\"Contextual Precision: {contextual_precision_results[i]['score']:.3f if contextual_precision_results[i]['score'] else 'N/A'} - {'✓' if contextual_precision_results[i]['success'] else '✗'}\")\n",
        "    \n",
        "    if i < len(contextual_recall_results):\n",
        "        print(f\"Contextual Recall: {contextual_recall_results[i]['score']:.3f if contextual_recall_results[i]['score'] else 'N/A'} - {'✓' if contextual_recall_results[i]['success'] else '✗'}\")\n",
        "    \n",
        "    if i < len(contextual_relevancy_results):\n",
        "        print(f\"Contextual Relevancy: {contextual_relevancy_results[i]['score']:.3f if contextual_relevancy_results[i]['score'] else 'N/A'} - {'✓' if contextual_relevancy_results[i]['success'] else '✗'}\")\n",
        "    \n",
        "    print(f\"\\nAnswer: {test_case.actual_output[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- **Faithfulness**: Measures if the answer is grounded in the retrieved context (no hallucinations)\n",
        "- **Answer Relevancy**: Measures if the answer is relevant to the question\n",
        "- **Contextual Precision**: Measures precision of retrieved context (how many retrieved chunks are relevant)\n",
        "- **Contextual Recall**: Measures recall of retrieved context (how much relevant information was retrieved)\n",
        "- **Contextual Relevancy**: Measures if the retrieved context is relevant to the question\n",
        "\n",
        "All metrics use a threshold of 0.7. A test case passes if the score is >= 0.7."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hair_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
